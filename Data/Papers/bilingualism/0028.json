{"authors": [["Zinszer", "Benjamin D", "BD", "Department of Communication Sciences and Disorders, University of Texas at Austin."], ["Rolotti", "Sebi V", "SV", "Department of Neuroscience, Columbia University."], ["Li", "Fan", "F", "Department of Biostatistics and Bioinformatics, Duke University."], ["Li", "Ping", "P", "Department of Psychology and Center for Brain, Behavior, and Cognition, Pennsylvania State University."]], "date": "2017-11-20", "id": "29154481", "text": "Infant language learners are faced with the difficult inductive problem of determining how new words map to novel or known objects in their environment. Bayesian inference models have been successful at using the sparse information available in natural child-directed speech to build candidate lexicons and infer speakers' referential intentions. We begin by asking how a Bayesian model optimized for monolingual input (the Intentional Model; Frank et\u00a0al., 2009) generalizes to new monolingual or bilingual corpora and find that, especially in the case of the bilingual input, the model shows a significant decrease in performance. In the next experiment, we propose the ME Model, a modified Bayesian model, which approximates infants' mutual exclusivity bias to support the differential demands of monolingual and bilingual learning situations. The extended model is assessed using the same corpora of real child-directed speech, showing that its performance is more robust against varying input and less dependent than the Intentional Model on optimization of its parsimony parameter. We argue that both monolingual and bilingual demands on word learning are important considerations for a computational model, as they can yield significantly different results than when only one such context is considered.", "doi": "10.1111/cogs.12567", "title": "Bayesian Word Learning in Multiple Language Environments.", "journal": ["Cognitive science", "Cogn Sci"]}