{"authors": [["Treille", "Avril", "A", "GIPSA-lab, D\u00e9partement Parole et Cognition, Universit\u00e9 Grenoble Alpes & CNRS, Grenoble, France."], ["Vilain", "Coriandre", "C", "GIPSA-lab, D\u00e9partement Parole et Cognition, Universit\u00e9 Grenoble Alpes & CNRS, Grenoble, France."], ["Schwartz", "Jean-Luc", "JL", "GIPSA-lab, D\u00e9partement Parole et Cognition, Universit\u00e9 Grenoble Alpes & CNRS, Grenoble, France."], ["Hueber", "Thomas", "T", "GIPSA-lab, D\u00e9partement Parole et Cognition, Universit\u00e9 Grenoble Alpes & CNRS, Grenoble, France."], ["Sato", "Marc", "M", "Laboratoire Parole et Langage, Aix-Marseille Universit\u00e9 & CNRS, Aix-en-Provence, France. Electronic address: marc.sato@lpl-aix.fr."]], "date": "2017-12-14", "id": "29248497", "text": "Recent neurophysiological studies demonstrate that audio-visual speech integration partly operates through temporal expectations and speech-specific predictions. From these results, one common view is that the binding of auditory and visual, lipread, speech cues relies on their joint probability and prior associative audio-visual experience. The present EEG study examined whether visual tongue movements integrate with relevant speech sounds, despite little associative audio-visual experience between the two modalities. A second objective was to determine possible similarities and differences of audio-visual speech integration between unusual audio-visuo-lingual and classical audio-visuo-labial modalities. To this aim, participants were presented with auditory, visual, and audio-visual isolated syllables, with the visual presentation related to either a sagittal view of the tongue movements or a facial view of the lip movements of a speaker, with lingual and facial movements previously recorded by an ultrasound imaging system and a video camera. In line with previous EEG studies, our results revealed an amplitude decrease and a latency facilitation of P2 auditory evoked potentials in both audio-visual-lingual and audio-visuo-labial conditions compared to the sum of unimodal conditions. These results argue against the view that auditory and visual speech cues solely integrate based on prior associative audio-visual perceptual experience. Rather, they suggest that dynamic and phonetic informational cues are sharable across sensory modalities, possibly through a cross-modal transfer of implicit articulatory motor knowledge.", "doi": "10.1016/j.neuropsychologia.2017.12.024", "title": "Electrophysiological evidence for Audio-visuo-lingual speech integration.", "journal": ["Neuropsychologia", "Neuropsychologia"]}