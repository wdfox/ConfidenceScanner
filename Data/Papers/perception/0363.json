{"authors": [["Havy", "M\u00e9lanie", "M", "Faculty of Psychology and Educational Sciences, University of Geneva, Geneva, Switzerland."], ["Zesiger", "Pascal", "P", "Faculty of Psychology and Educational Sciences, University of Geneva, Geneva, Switzerland."]], "date": "2017-12-08", "id": "29276493", "text": "From the very first moments of their lives, infants are able to link specific movements of the visual articulators to auditory speech signals. However, recent evidence indicates that infants focus primarily on auditory speech signals when learning new words. Here, we ask whether 30-month-old children are able to learn new words based solely on visible speech information, and whether information from both auditory and visual modalities is available after learning in only one modality. To test this, children were taught new lexical mappings. One group of children experienced the words in the auditory modality (i.e., acoustic form of the word with no accompanying face). Another group experienced the words in the visual modality (seeing a silent talking face). Lexical recognition was tested in either the learning modality or in the other modality. Results revealed successful word learning in either modality. Results further showed cross-modal recognition following an auditory-only, but not a visual-only, experience of the words. Together, these findings suggest that visible speech becomes increasingly informative for the purpose of lexical learning, but that an auditory-only experience evokes a cross-modal representation of the words.", "doi": "10.3389/fpsyg.2017.02122", "title": "Learning Spoken Words via the Ears and Eyes: Evidence from 30-Month-Old Children.", "journal": ["Frontiers in psychology", "Front Psychol"]}